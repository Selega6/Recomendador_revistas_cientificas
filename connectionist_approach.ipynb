{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "249ca394",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trabajando con: cpu\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from kornia.losses import focal_loss\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from torch.utils.data import DataLoader, WeightedRandomSampler\n",
    "\n",
    "from models.hybrid_CNN_BiLSTM import HybridCNNBiLSTM\n",
    "from models.simple_biLSTM import BiLSTMClassifier\n",
    "from src.dataset_factory import DatasetFactory\n",
    "from src.nn_trainer import NNTrainer\n",
    "from src.nn_utils import JournalDataset, build_vocab\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Trabajando con: {device}\")\n",
    "\n",
    "\n",
    "factory = DatasetFactory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8291031",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def perform_downsampling(df, target_column='label_idx'):\n",
    "    \n",
    "    counts = df[target_column].value_counts()\n",
    "    min_class_size = counts.min() \n",
    "    \n",
    "    target_size = max(min_class_size, 300) \n",
    "    \n",
    "    downsampled_parts = []\n",
    "    \n",
    "    for class_index in counts.index:\n",
    "        class_subset = df[df[target_column] == class_index]\n",
    "        \n",
    "        if len(class_subset) > target_size:\n",
    "            \n",
    "            downsampled_parts.append(class_subset.sample(target_size, random_state=42))\n",
    "        else:\n",
    "            \n",
    "            downsampled_parts.append(class_subset)\n",
    "            \n",
    "    return pd.concat(downsampled_parts).sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "\n",
    "def perform_soft_downsampling(df, target_column='label_idx', ratio=2.0, min_floor=300):\n",
    "    counts = df[target_column].value_counts()\n",
    "    min_class_size = counts.min()\n",
    "    \n",
    "    \n",
    "    base_size = max(min_class_size, min_floor)\n",
    "    \n",
    "    \n",
    "    \n",
    "    max_allowed_size = int(base_size * ratio)\n",
    "    \n",
    "    downsampled_parts = []\n",
    "    \n",
    "    for class_index in counts.index:\n",
    "        class_subset = df[df[target_column] == class_index]\n",
    "        n_samples = len(class_subset)\n",
    "        \n",
    "        if n_samples > max_allowed_size:\n",
    "            \n",
    "            downsampled_parts.append(class_subset.sample(max_allowed_size, random_state=42))\n",
    "        else:\n",
    "            \n",
    "            downsampled_parts.append(class_subset)\n",
    "            \n",
    "    return pd.concat(downsampled_parts).sample(frac=1, random_state=42).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16882da0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generando dataset DL para experimento: DL_Experiment_Base_aggresive...\n",
      "Dataset DL y mapeo guardados en data/experiments/DL_Experiment_Base_aggresive\n",
      "Distribución tras Downsampling:\n",
      " label_idx\n",
      "2    7168\n",
      "4    3162\n",
      "1    1725\n",
      "3     796\n",
      "0     787\n",
      "5     590\n",
      "Name: count, dtype: int64\n",
      "Tamaño total del vocabulario: 3002\n",
      "Muestra del vocabulario: [('proposed', 2), ('model', 3), ('data', 4), ('method', 5), ('learning', 6), ('results', 7), ('performance', 8), ('methods', 9), ('network', 10), ('based', 11)]\n"
     ]
    }
   ],
   "source": [
    "df = factory.create_dl_dataset('DL_Experiment_Base_aggresive')\n",
    "\n",
    "\n",
    "TARGET_TEST_SIZE = 0.15\n",
    "TARGET_VAL_SIZE = 0.15\n",
    "\n",
    "\n",
    "train_val_df, test_df = train_test_split(\n",
    "    df, test_size=TARGET_TEST_SIZE, stratify=df['label_idx'], random_state=42\n",
    ")\n",
    "\n",
    "val_relative_size = TARGET_VAL_SIZE / (1 - TARGET_TEST_SIZE)\n",
    "train_df, val_df = train_test_split(\n",
    "    train_val_df, test_size=val_relative_size, stratify=train_val_df['label_idx'], random_state=42\n",
    ")\n",
    "\n",
    "\n",
    "print(\"Distribution after Downsampling:\\n\", train_df['label_idx'].value_counts())\n",
    "\n",
    "\n",
    "vocab = build_vocab(train_df['processed_text'], max_features= 3000)\n",
    "\n",
    "print(f\"Total vocabulary size: {len(vocab)}\")\n",
    "\n",
    "print(\"Vocab sample:\", list(vocab.items())[:10])\n",
    "\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    JournalDataset(train_df['processed_text'], train_df['label_idx'], vocab), \n",
    "    batch_size=32, shuffle=True\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(JournalDataset(val_df['processed_text'], val_df['label_idx'], vocab), batch_size=32, shuffle=False)\n",
    "test_loader = DataLoader(JournalDataset(test_df['processed_text'], test_df['label_idx'], vocab), batch_size=32, shuffle=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "460db73b",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = lambda preds, targets: focal_loss(preds, targets, alpha=0.5, gamma=2.0, reduction='mean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a9f1ae4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iniciando entrenamiento en cpu...\n",
      "Epoch 01/30 | Loss: 0.063 | Val Acc: 0.562 -> ¡Mejor modelo guardado!\n",
      "Epoch 02/30 | Loss: 0.046 | Val Acc: 0.618 -> ¡Mejor modelo guardado!\n",
      "Epoch 03/30 | Loss: 0.039 | Val Acc: 0.648 -> ¡Mejor modelo guardado!\n",
      "Epoch 04/30 | Loss: 0.036 | Val Acc: 0.671 -> ¡Mejor modelo guardado!\n",
      "Epoch 05/30 | Loss: 0.033 | Val Acc: 0.671\n",
      "Epoch 06/30 | Loss: 0.030 | Val Acc: 0.676 -> ¡Mejor modelo guardado!\n",
      "Epoch 07/30 | Loss: 0.028 | Val Acc: 0.684 -> ¡Mejor modelo guardado!\n",
      "Epoch 08/30 | Loss: 0.027 | Val Acc: 0.681\n",
      "Epoch 09/30 | Loss: 0.025 | Val Acc: 0.693 -> ¡Mejor modelo guardado!\n",
      "Epoch 10/30 | Loss: 0.024 | Val Acc: 0.694 -> ¡Mejor modelo guardado!\n",
      "Epoch 11/30 | Loss: 0.022 | Val Acc: 0.688\n",
      "Epoch 12/30 | Loss: 0.020 | Val Acc: 0.691\n",
      "Epoch 13/30 | Loss: 0.019 | Val Acc: 0.647\n",
      "Epoch 14/30 | Loss: 0.015 | Val Acc: 0.696 -> ¡Mejor modelo guardado!\n",
      "Epoch 15/30 | Loss: 0.014 | Val Acc: 0.691\n",
      "Epoch 16/30 | Loss: 0.014 | Val Acc: 0.695\n",
      "Epoch 17/30 | Loss: 0.013 | Val Acc: 0.696 -> ¡Mejor modelo guardado!\n",
      "Epoch 18/30 | Loss: 0.013 | Val Acc: 0.690\n",
      "Epoch 19/30 | Loss: 0.013 | Val Acc: 0.693\n",
      "Epoch 20/30 | Loss: 0.012 | Val Acc: 0.689\n",
      "Epoch 21/30 | Loss: 0.012 | Val Acc: 0.689\n",
      "Epoch 22/30 | Loss: 0.012 | Val Acc: 0.689\n",
      "\n",
      "[Early Stopping] El modelo no ha mejorado en 5 épocas. Deteniendo...\n",
      "Test Accuracy: 0.707\n",
      "Resultados guardados en data\\experiments\\BiLSTM_Simple_V11\n"
     ]
    }
   ],
   "source": [
    "\n",
    "exp_path_simple = os.path.join('data', 'experiments', 'BiLSTM_Simple_V11')\n",
    "os.makedirs(exp_path_simple, exist_ok=True)\n",
    "\n",
    "model_simple = BiLSTMClassifier(\n",
    "    vocab_size=len(vocab), \n",
    "    embed_dim=128, \n",
    "    hidden_dim=128, \n",
    "    output_dim=df['label_idx'].nunique()\n",
    ").to(device)\n",
    "\n",
    "optimizer = optim.Adam(model_simple.parameters(), lr=0.001, weight_decay=1e-5)\n",
    "\n",
    "mode='max' \n",
    "factor=0.1 \n",
    "patience=2 \n",
    "scheduler = ReduceLROnPlateau(optimizer, mode=mode, factor=factor, patience=patience)\n",
    "\n",
    "\n",
    "trainer_simple = NNTrainer(\n",
    "    model=model_simple, \n",
    "    criterion=criterion, \n",
    "    optimizer=optimizer, \n",
    "    device=device, \n",
    "    exp_path=exp_path_simple,\n",
    "    scheduler=scheduler \n",
    ")\n",
    "trainer_simple.fit(train_loader, val_loader, epochs=30)\n",
    "\n",
    "\n",
    "y_true, y_pred, acc = trainer_simple.evaluate(test_loader)\n",
    "print(f\"Test Accuracy: {acc:.3f}\")\n",
    "\n",
    "trainer_simple.save_results(y_true, y_pred, vocab)\n",
    "\n",
    "trainer_simple.plot_learning_curves()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "702b801d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iniciando entrenamiento en cpu...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.6 and num_layers=1\n",
      "  warnings.warn(\n",
      "c:\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:366: UserWarning: Using padding='same' with even kernel lengths and odd dilation may require a zero-padded copy of the input be created (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\aten\\src\\ATen\\native\\Convolution.cpp:1028.)\n",
      "  return F.conv1d(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01/50 | Loss: 0.070 | Val Acc: 0.531 -> ¡Mejor modelo guardado!\n",
      "Epoch 02/50 | Loss: 0.058 | Val Acc: 0.595 -> ¡Mejor modelo guardado!\n",
      "Epoch 03/50 | Loss: 0.050 | Val Acc: 0.613 -> ¡Mejor modelo guardado!\n",
      "Epoch 04/50 | Loss: 0.043 | Val Acc: 0.637 -> ¡Mejor modelo guardado!\n",
      "Epoch 05/50 | Loss: 0.040 | Val Acc: 0.654 -> ¡Mejor modelo guardado!\n",
      "Epoch 06/50 | Loss: 0.037 | Val Acc: 0.657 -> ¡Mejor modelo guardado!\n",
      "Epoch 07/50 | Loss: 0.034 | Val Acc: 0.674 -> ¡Mejor modelo guardado!\n",
      "Epoch 08/50 | Loss: 0.032 | Val Acc: 0.679 -> ¡Mejor modelo guardado!\n",
      "Epoch 09/50 | Loss: 0.030 | Val Acc: 0.681 -> ¡Mejor modelo guardado!\n",
      "Epoch 10/50 | Loss: 0.027 | Val Acc: 0.683 -> ¡Mejor modelo guardado!\n",
      "Epoch 11/50 | Loss: 0.027 | Val Acc: 0.672\n",
      "Epoch 12/50 | Loss: 0.024 | Val Acc: 0.690 -> ¡Mejor modelo guardado!\n",
      "Epoch 13/50 | Loss: 0.022 | Val Acc: 0.689\n",
      "Epoch 14/50 | Loss: 0.021 | Val Acc: 0.678\n",
      "Epoch 15/50 | Loss: 0.019 | Val Acc: 0.688\n",
      "\n",
      "[Early Stopping] El modelo no ha mejorado en 3 épocas. Deteniendo...\n"
     ]
    }
   ],
   "source": [
    "\n",
    "exp_path_hybrid = os.path.join('data', 'experiments', 'Hybrid_CNN_BiLSTM_V7')\n",
    "os.makedirs(exp_path_hybrid, exist_ok=True)\n",
    "\n",
    "model_hybrid = HybridCNNBiLSTM(\n",
    "    vocab_size=len(vocab), \n",
    "    embed_dim=128,      \n",
    "    n_filters=100,       \n",
    "    filter_sizes=[3, 4, 5], \n",
    "    hidden_dim=32,      \n",
    "    output_dim=df['label_idx'].nunique(),\n",
    "    dropout=0.6         \n",
    ").to(device)\n",
    "\n",
    "\n",
    "optimizer_h = optim.Adam(\n",
    "    model_hybrid.parameters(), \n",
    "    lr=0.001, \n",
    "    weight_decay=1e-5  \n",
    ")\n",
    "\n",
    "\n",
    "scheduler = ReduceLROnPlateau(optimizer_h, mode='max', factor=0.5, patience=1)\n",
    "trainer_hybrid = NNTrainer(\n",
    "    model=model_hybrid, \n",
    "    criterion=criterion, \n",
    "    optimizer=optimizer_h, \n",
    "    device=device, \n",
    "    exp_path=exp_path_hybrid,\n",
    "    scheduler=scheduler\n",
    ")\n",
    "\n",
    "\n",
    "trainer_hybrid.fit(train_loader, val_loader, epochs=50, patience=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ca81fd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resultados guardados en data\\experiments\\Hybrid_CNN_BiLSTM_V6\n",
      "Test Accuracy Hybrid: 0.691\n"
     ]
    }
   ],
   "source": [
    "\n",
    "y_true_h, y_pred_h, acc_h = trainer_hybrid.evaluate(test_loader)\n",
    "trainer_hybrid.save_results(y_true_h, y_pred_h, vocab)\n",
    "print(f\"Test Accuracy Hybrid: {acc_h:.3f}\")\n",
    "trainer_hybrid.plot_learning_curves()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
