{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "249ca394",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trabajando con: cpu\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from kornia.losses import focal_loss\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from torch.utils.data import DataLoader, WeightedRandomSampler\n",
    "\n",
    "from models.hybrid_CNN_BiLSTM import HybridCNNBiLSTM\n",
    "from models.simple_biLSTM import BiLSTMClassifier\n",
    "from src.dataset_factory import DatasetFactory\n",
    "from src.nn_trainer import NNTrainer\n",
    "from src.nn_utils import JournalDataset, build_vocab\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Trabajando con: {device}\")\n",
    "\n",
    "\n",
    "factory = DatasetFactory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b8291031",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def perform_downsampling(df, target_column='label_idx'):\n",
    "    \n",
    "    counts = df[target_column].value_counts()\n",
    "    min_class_size = counts.min() \n",
    "    \n",
    "    target_size = max(min_class_size, 300) \n",
    "    \n",
    "    downsampled_parts = []\n",
    "    \n",
    "    for class_index in counts.index:\n",
    "        class_subset = df[df[target_column] == class_index]\n",
    "        \n",
    "        if len(class_subset) > target_size:\n",
    "            \n",
    "            downsampled_parts.append(class_subset.sample(target_size, random_state=42))\n",
    "        else:\n",
    "            \n",
    "            downsampled_parts.append(class_subset)\n",
    "            \n",
    "    return pd.concat(downsampled_parts).sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "\n",
    "def perform_soft_downsampling(df, target_column='label_idx', ratio=2.0, min_floor=300):\n",
    "    counts = df[target_column].value_counts()\n",
    "    min_class_size = counts.min()\n",
    "    \n",
    "    \n",
    "    base_size = max(min_class_size, min_floor)\n",
    "    \n",
    "    \n",
    "    \n",
    "    max_allowed_size = int(base_size * ratio)\n",
    "    \n",
    "    downsampled_parts = []\n",
    "    \n",
    "    for class_index in counts.index:\n",
    "        class_subset = df[df[target_column] == class_index]\n",
    "        n_samples = len(class_subset)\n",
    "        \n",
    "        if n_samples > max_allowed_size:\n",
    "            \n",
    "            downsampled_parts.append(class_subset.sample(max_allowed_size, random_state=42))\n",
    "        else:\n",
    "            \n",
    "            downsampled_parts.append(class_subset)\n",
    "            \n",
    "    return pd.concat(downsampled_parts).sample(frac=1, random_state=42).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "16882da0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating DL dataset for experiment: DL_Experiment_Base_aggresive...\n",
      "Dataset DL and mapping saved in data/experiments/DL_Experiment_Base_aggresive\n",
      "Distribution after Downsampling:\n",
      " label_idx\n",
      "1    7169\n",
      "2    3162\n",
      "0     787\n",
      "3     590\n",
      "Name: count, dtype: int64\n",
      "Total vocabulary size: 3002\n",
      "Vocab sample: [('proposed', 2), ('model', 3), ('data', 4), ('method', 5), ('results', 6), ('learning', 7), ('performance', 8), ('methods', 9), ('based', 10), ('information', 11)]\n"
     ]
    }
   ],
   "source": [
    "df = factory.create_dl_dataset('DL_Experiment_Base_aggresive')\n",
    "\n",
    "\n",
    "TARGET_TEST_SIZE = 0.15\n",
    "TARGET_VAL_SIZE = 0.15\n",
    "\n",
    "\n",
    "train_val_df, test_df = train_test_split(\n",
    "    df, test_size=TARGET_TEST_SIZE, stratify=df['label_idx'], random_state=42\n",
    ")\n",
    "\n",
    "val_relative_size = TARGET_VAL_SIZE / (1 - TARGET_TEST_SIZE)\n",
    "train_df, val_df = train_test_split(\n",
    "    train_val_df, test_size=val_relative_size, stratify=train_val_df['label_idx'], random_state=42\n",
    ")\n",
    "\n",
    "#perform downsampling\n",
    "#train_df = perform_downsampling(train_df, target_column='label_idx')\n",
    "\n",
    "print(\"Distribution after Downsampling:\\n\", train_df['label_idx'].value_counts())\n",
    "\n",
    "\n",
    "vocab = build_vocab(train_df['processed_text'], max_features= 3000)\n",
    "\n",
    "print(f\"Total vocabulary size: {len(vocab)}\")\n",
    "\n",
    "print(\"Vocab sample:\", list(vocab.items())[:10])\n",
    "\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    JournalDataset(train_df['processed_text'], train_df['label_idx'], vocab), \n",
    "    batch_size=32, shuffle=True\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(JournalDataset(val_df['processed_text'], val_df['label_idx'], vocab), batch_size=32, shuffle=False)\n",
    "test_loader = DataLoader(JournalDataset(test_df['processed_text'], test_df['label_idx'], vocab), batch_size=32, shuffle=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "460db73b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#criterion = lambda preds, targets: focal_loss(preds, targets, alpha=0.5, gamma=2.0, reduction='mean')\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2a9f1ae4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training on cpu...\n",
      "Epoch 01/30 | Loss: 0.065 | Val Acc: 0.617 -> Best model saved\n",
      "Epoch 02/30 | Loss: 0.052 | Val Acc: 0.654 -> Best model saved\n",
      "Epoch 03/30 | Loss: 0.041 | Val Acc: 0.693 -> Best model saved\n",
      "Epoch 04/30 | Loss: 0.035 | Val Acc: 0.708 -> Best model saved\n",
      "Epoch 05/30 | Loss: 0.030 | Val Acc: 0.757 -> Best model saved\n",
      "Epoch 06/30 | Loss: 0.026 | Val Acc: 0.759 -> Best model saved\n",
      "Epoch 07/30 | Loss: 0.025 | Val Acc: 0.796 -> Best model saved\n",
      "Epoch 08/30 | Loss: 0.022 | Val Acc: 0.796\n",
      "Epoch 09/30 | Loss: 0.020 | Val Acc: 0.803 -> Best model saved\n",
      "Epoch 10/30 | Loss: 0.018 | Val Acc: 0.809 -> Best model saved\n",
      "Epoch 11/30 | Loss: 0.017 | Val Acc: 0.812 -> Best model saved\n",
      "Epoch 12/30 | Loss: 0.016 | Val Acc: 0.806\n",
      "Epoch 13/30 | Loss: 0.015 | Val Acc: 0.807\n",
      "Epoch 14/30 | Loss: 0.014 | Val Acc: 0.815 -> Best model saved\n",
      "Epoch 15/30 | Loss: 0.014 | Val Acc: 0.811\n",
      "Epoch 16/30 | Loss: 0.013 | Val Acc: 0.819 -> Best model saved\n",
      "Epoch 17/30 | Loss: 0.012 | Val Acc: 0.817\n",
      "Epoch 18/30 | Loss: 0.012 | Val Acc: 0.817\n",
      "Epoch 19/30 | Loss: 0.011 | Val Acc: 0.819 -> Best model saved\n",
      "Epoch 20/30 | Loss: 0.011 | Val Acc: 0.805\n",
      "Epoch 21/30 | Loss: 0.011 | Val Acc: 0.807\n",
      "Epoch 22/30 | Loss: 0.011 | Val Acc: 0.809\n",
      "Epoch 23/30 | Loss: 0.009 | Val Acc: 0.815\n",
      "Epoch 24/30 | Loss: 0.008 | Val Acc: 0.820 -> Best model saved\n",
      "Epoch 25/30 | Loss: 0.008 | Val Acc: 0.818\n",
      "Epoch 26/30 | Loss: 0.008 | Val Acc: 0.819\n",
      "Epoch 27/30 | Loss: 0.008 | Val Acc: 0.815\n",
      "Epoch 28/30 | Loss: 0.008 | Val Acc: 0.818\n",
      "Epoch 29/30 | Loss: 0.008 | Val Acc: 0.818\n",
      "\n",
      "[Early Stopping] The model has not improved in 5 epochs. Stopping...\n",
      "Test Accuracy: 0.810\n",
      "Results saved in data\\experiments\\BiLSTM_Simple_V11\n"
     ]
    }
   ],
   "source": [
    "\n",
    "exp_path_simple = os.path.join('data', 'experiments', 'BiLSTM_Simple_V11')\n",
    "os.makedirs(exp_path_simple, exist_ok=True)\n",
    "\n",
    "model_simple = BiLSTMClassifier(\n",
    "    vocab_size=len(vocab), \n",
    "    embed_dim=128, \n",
    "    hidden_dim=32, \n",
    "    output_dim=df['label_idx'].nunique()\n",
    ").to(device)\n",
    "\n",
    "optimizer = optim.Adam(model_simple.parameters(), lr=0.001, weight_decay=1e-5)\n",
    "\n",
    "mode='max' \n",
    "factor=0.1 \n",
    "patience=2 \n",
    "scheduler = ReduceLROnPlateau(optimizer, mode=mode, factor=factor, patience=patience)\n",
    "\n",
    "\n",
    "trainer_simple = NNTrainer(\n",
    "    model=model_simple, \n",
    "    criterion=criterion, \n",
    "    optimizer=optimizer, \n",
    "    device=device, \n",
    "    exp_path=exp_path_simple,\n",
    "    scheduler=scheduler \n",
    ")\n",
    "trainer_simple.fit(train_loader, val_loader, epochs=30)\n",
    "\n",
    "\n",
    "y_true, y_pred, acc = trainer_simple.evaluate(test_loader)\n",
    "print(f\"Test Accuracy: {acc:.3f}\")\n",
    "\n",
    "trainer_simple.save_results(y_true, y_pred, vocab)\n",
    "\n",
    "trainer_simple.plot_learning_curves()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "702b801d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training on cpu...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.6 and num_layers=1\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01/50 | Loss: 0.963 | Val Acc: 0.618 -> Best model saved\n",
      "Epoch 02/50 | Loss: 0.791 | Val Acc: 0.682 -> Best model saved\n",
      "Epoch 03/50 | Loss: 0.677 | Val Acc: 0.750 -> Best model saved\n",
      "Epoch 04/50 | Loss: 0.583 | Val Acc: 0.785 -> Best model saved\n",
      "Epoch 05/50 | Loss: 0.537 | Val Acc: 0.784\n",
      "Epoch 06/50 | Loss: 0.485 | Val Acc: 0.807 -> Best model saved\n",
      "Epoch 07/50 | Loss: 0.449 | Val Acc: 0.807\n",
      "Epoch 08/50 | Loss: 0.414 | Val Acc: 0.802\n",
      "Epoch 09/50 | Loss: 0.387 | Val Acc: 0.808 -> Best model saved\n",
      "Epoch 10/50 | Loss: 0.354 | Val Acc: 0.796\n",
      "Epoch 11/50 | Loss: 0.340 | Val Acc: 0.810 -> Best model saved\n",
      "Epoch 12/50 | Loss: 0.308 | Val Acc: 0.795\n",
      "Epoch 13/50 | Loss: 0.303 | Val Acc: 0.801\n",
      "Epoch 14/50 | Loss: 0.278 | Val Acc: 0.812 -> Best model saved\n",
      "Epoch 15/50 | Loss: 0.250 | Val Acc: 0.808\n",
      "Epoch 16/50 | Loss: 0.231 | Val Acc: 0.804\n",
      "Epoch 17/50 | Loss: 0.223 | Val Acc: 0.809\n",
      "Epoch 18/50 | Loss: 0.180 | Val Acc: 0.808\n",
      "Epoch 19/50 | Loss: 0.173 | Val Acc: 0.810\n",
      "\n",
      "[Early Stopping] The model has not improved in 5 epochs. Stopping...\n"
     ]
    }
   ],
   "source": [
    "\n",
    "exp_path_hybrid = os.path.join('data', 'experiments', 'Hybrid_CNN_BiLSTM_V11')\n",
    "os.makedirs(exp_path_hybrid, exist_ok=True)\n",
    "\n",
    "model_hybrid = HybridCNNBiLSTM(\n",
    "    vocab_size=len(vocab), \n",
    "    embed_dim=128,      \n",
    "    n_filters=100,       \n",
    "    filter_sizes=[3, 4, 5], \n",
    "    hidden_dim=32,      \n",
    "    output_dim=df['label_idx'].nunique(),\n",
    "    dropout=0.6         \n",
    ").to(device)\n",
    "\n",
    "\n",
    "optimizer_h = optim.Adam(\n",
    "    model_hybrid.parameters(), \n",
    "    lr=0.001, \n",
    "    weight_decay=1e-5  \n",
    ")\n",
    "\n",
    "\n",
    "scheduler = ReduceLROnPlateau(optimizer_h, mode=mode, factor=factor, patience=patience)\n",
    "trainer_hybrid = NNTrainer(\n",
    "    model=model_hybrid, \n",
    "    criterion=criterion, \n",
    "    optimizer=optimizer_h, \n",
    "    device=device, \n",
    "    exp_path=exp_path_hybrid,\n",
    "    scheduler=scheduler\n",
    ")\n",
    "\n",
    "\n",
    "trainer_hybrid.fit(train_loader, val_loader, epochs=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "5ca81fd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results saved in data\\experiments\\Hybrid_CNN_BiLSTM_V11\n",
      "Test Accuracy Hybrid: 0.807\n"
     ]
    }
   ],
   "source": [
    "\n",
    "y_true_h, y_pred_h, acc_h = trainer_hybrid.evaluate(test_loader)\n",
    "trainer_hybrid.save_results(y_true_h, y_pred_h, vocab)\n",
    "print(f\"Test Accuracy Hybrid: {acc_h:.3f}\")\n",
    "trainer_hybrid.plot_learning_curves()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
